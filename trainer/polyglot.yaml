general:
  model_name: 'polyglot'
  data_path: 'amphora/plat-clean'
  freezing_strategy: 'intermediate'
  base_model_path: 'EleutherAI/polyglot-ko-12.8b'
  wandb_api_key: '559c4e6dd6a72ffc6875f4cd60095ac1a6518b37'
  wandb_project: 'polyglot-instruct'
  output_dir: 'ckpt/'

training:
  num_train_epochs: 1
  per_device_train_batch_size: 6
  gradient_accumulation_steps: 30
  logging_steps: 10
  save_strategy: 'epoch'
  save_steps: 100
  max_seq_length: 2048
  packing: true
  dataset_text_field: 'text'
  # Add more parameters as needed

freeze_parameters:
  freeze_ratio: 0.25
  # Add more parameters as needed
